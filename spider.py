from bs4 import BeautifulSoup
import urllib.request
import pandas
from concurrent.futures import ThreadPoolExecutor

headers = {'User-Agent': 'mozilla/5.0 (windows nt 6.1; wow64) applewebkit/537.36 (khtml, like gecko) chrome/27.0.1453.94 safari/537.36'}

data = pandas.read_csv('ContractDefects.csv')
data = data.values.tolist()
filenames = []
drawbacks = []
for i in data:
     temp = []
     filenames.append(i[0])
     for j in range(1, 21):
          if i[j]==1 or i[j]=='1':
               temp.append(j)
     drawbacks.append(temp)
# print(filenames)

def workArray(target):
     res = ''
     for i in target:
          res+=str(i)
          if i!= target[-1]:
               res+="_"
     return res
#beautiful soup
def spider(filename, drawback):
     html_doc = "https://etherscan.io/address/"+filename

     req = urllib.request.Request(html_doc, headers=headers)
     webpage = urllib.request.urlopen(req)
     html = webpage.read()

     soup = BeautifulSoup(html, 'html.parser')
     content = ''
     for k in soup.find_all('pre', class_='js-sourcecopyarea'):  # ,string='更多'
          content += k.string
     filepath = './file/' + filename + workArray(drawback) + '.sol'
     print(filepath, '已完成')

     with open(filepath, 'w') as f:
          f.write(content)


executor = ThreadPoolExecutor(max_workers=10)
for i in range(len(filenames)):
     executor.submit(spider, filenames[i], drawbacks[i])
